{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27cb0f5",
   "metadata": {},
   "source": [
    "# Task 2: Bayesian Change Point Detection\n",
    "\n",
    "## Objective\n",
    "Detect structural breaks in the Brent oil price time series using a Bayesian Change Point model. We will identify the date of significant changes in the mean of log returns (or volatility) and associate them with historical events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f53180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.data_loader import load_data, calculate_log_returns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "print(f\"PyMC Version: {pm.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb080b1",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Load the data and calculate log returns. Log returns are often preferred for financial time series modeling as they are more likely to be stationary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = '../data/BrentOilPrices.csv'\n",
    "df = load_data(file_path)\n",
    "df = calculate_log_returns(df)\n",
    "\n",
    "# Visualize Log Returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df.index, df['Log_Returns'], alpha=0.6)\n",
    "plt.title('Brent Oil Price Log Returns')\n",
    "plt.ylabel('Log Return')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af3843",
   "metadata": {},
   "source": [
    "## 2. Bayesian Change Point Model\n",
    "We will model the log returns as coming from two different Gaussian distributions, separated by a change point $\\tau$.\n",
    "\n",
    "$$\n",
    "y_t \\sim \\mathcal{N}(\\mu_t, \\sigma) \\\\mu_t = \\begin{cases} \\mu_1 & \\text{if } t < \\tau \\\\ \\mu_2 & \\text{if } t \\ge \\tau \\end{cases}\n",
    "$$\n",
    "\n",
    "Priors:\n",
    "- $\\tau \\sim \\text{DiscreteUniform}(0, T)$\n",
    "- $\\mu_1, \\mu_2 \\sim \\mathcal{N}(0, 0.1)$\n",
    "- $\\sigma \\sim \\text{HalfNormal}(0.1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyMC\n",
    "y = df['Log_Returns'].values\n",
    "n_samples = len(y)\n",
    "idx = np.arange(n_samples)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_samples - 1)\n",
    "    mu1 = pm.Normal(\"mu1\", mu=0, sigma=0.1)\n",
    "    mu2 = pm.Normal(\"mu2\", mu=0, sigma=0.1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n",
    "    \n",
    "    # Switch function\n",
    "    mu = pm.math.switch(tau >= idx, mu2, mu1)\n",
    "    \n",
    "    # Likelihood\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=y)\n",
    "    \n",
    "    # Sampling\n",
    "    trace = pm.sample(1000, tune=1000, cores=1, return_inferencedata=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58503f22",
   "metadata": {},
   "source": [
    "## 3. Model Interpretation\n",
    "Check convergence and visualize the posterior distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99797820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace Plot\n",
    "az.plot_trace(trace)\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "summary = az.summary(trace)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ab0b5",
   "metadata": {},
   "source": [
    "## 4. Identifying the Change Point\n",
    "Extract the posterior distribution of $\\tau$ and map it back to the original dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior of Tau\n",
    "tau_posterior = trace.posterior['tau'].values.flatten()\n",
    "tau_mean = int(tau_posterior.mean())\n",
    "\n",
    "# Map back to date\n",
    "change_date = df.index[tau_mean]\n",
    "print(f\"Detected Change Point Date: {change_date.date()}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df.index, df['Log_Returns'], alpha=0.5, label='Log Returns')\n",
    "plt.axvline(change_date, color='red', linestyle='--', label=f'Change Point ({change_date.date()})')\n",
    "plt.legend()\n",
    "plt.title('Detected Change Point in Log Returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee911fa",
   "metadata": {},
   "source": [
    "## 5. Event Association\n",
    "Load the compiled events data and check for proximity to the detected change point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Events\n",
    "events_df = pd.read_csv('../data/events_data.csv')\n",
    "events_df['Date'] = pd.to_datetime(events_df['Date'])\n",
    "\n",
    "# Find closest events\n",
    "events_df['Days_Diff'] = (events_df['Date'] - change_date).dt.days.abs()\n",
    "closest_events = events_df.sort_values('Days_Diff').head(3)\n",
    "\n",
    "print(\"Closest Historical Events to Detected Change Point:\")\n",
    "print(closest_events[['Date', 'Event', 'Description', 'Days_Diff']])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
